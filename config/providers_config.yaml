# Multi-Provider Evaluation Configuration
# This file defines multiple providers and models to run evaluations against
# Format: provider_name: [list of models]

# OpenAI Provider
openai:
  - "gpt-4o-mini"
  # - "gpt-4-turbo"  # Commented out for faster testing

# Watsonx Provider (example - uncomment to use)
# watsonx:
#   - "ibm/granite-13b-chat-v2"
#   - "meta-llama/llama-3-70b-instruct"

# Gemini Provider (example - uncomment to use)
# gemini:
#   - "gemini-1.5-pro"
#   - "gemini-1.5-flash"

# Hosted VLLM Provider (example - uncomment to use)
# hosted_vllm:
#   - "meta-llama/Meta-Llama-3-8B-Instruct"

# Global settings (optional)
settings:
  # Output directory structure: {output_base}/{provider}/{model}/
  output_base: "./eval_output"

